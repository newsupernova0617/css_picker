# ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” - CSS Picker Turso ì¸ë±ìŠ¤ ìµœì í™” ë° ì„±ëŠ¥ ë¶„ì„
# Turso SQLite í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í¬ê´„ì  ìµœì í™” ë„êµ¬

import os
import sys
import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import libsql as libsql
from contextlib import contextmanager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('database_optimization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class QueryAnalysis:
    """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼"""
    query: str
    execution_time_ms: float
    rows_affected: int
    explanation: str
    optimization_suggestions: List[str]
    index_candidates: List[str]
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL

@dataclass
class IndexRecommendation:
    """ì¸ë±ìŠ¤ ì¶”ì²œ ê²°ê³¼"""
    table_name: str
    columns: List[str]
    index_type: str  # UNIQUE, COMPOSITE, SINGLE, PARTIAL
    estimated_improvement: float
    creation_cost: str
    maintenance_overhead: str
    justification: str

@dataclass
class OptimizationReport:
    """ìµœì í™” ë³´ê³ ì„œ"""
    timestamp: str
    database_size_mb: float
    total_tables: int
    total_indexes: int
    slow_queries: List[QueryAnalysis]
    index_recommendations: List[IndexRecommendation]
    performance_metrics: Dict[str, Any]
    backup_info: Dict[str, Any]

class TursoOptimizer:
    """
    Turso ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë„êµ¬
    - ì¸ë±ìŠ¤ ìµœì í™” ë° ì¶”ì²œ
    - ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
    - ìë™ ë°±ì—… ì‹œìŠ¤í…œ
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, database_url: str, auth_token: str):
        self.database_url = database_url
        self.auth_token = auth_token
        self.db = None
        self.performance_baseline = {}
        self.connect()
    
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.db = libsql.connect(self.database_url, auth_token=self.auth_token)
            logger.info("Turso ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    @contextmanager
    def get_cursor(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì•ˆì „í•œ ì»¤ì„œ ê´€ë¦¬"""
        cursor = self.db.cursor()
        try:
            yield cursor
        finally:
            cursor.close()
    
    def analyze_current_schema(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„"""
        schema_info = {
            'tables': {},
            'indexes': {},
            'statistics': {}
        }
        
        try:
            with self.get_cursor() as cursor:
                # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
                cursor.execute("""
                    SELECT name, sql FROM sqlite_master 
                    WHERE type = 'table' AND name NOT LIKE 'sqlite_%'
                    ORDER BY name
                """)
                
                for table_name, create_sql in cursor.fetchall():
                    # í…Œì´ë¸” í†µê³„
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    row_count = cursor.fetchone()[0]
                    
                    # ì»¬ëŸ¼ ì •ë³´
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    columns = cursor.fetchall()
                    
                    schema_info['tables'][table_name] = {
                        'create_sql': create_sql,
                        'row_count': row_count,
                        'columns': columns
                    }
                
                # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute("""
                    SELECT name, tbl_name, sql FROM sqlite_master 
                    WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                    ORDER BY tbl_name, name
                """)
                
                for index_name, table_name, create_sql in cursor.fetchall():
                    if table_name not in schema_info['indexes']:
                        schema_info['indexes'][table_name] = []
                    
                    schema_info['indexes'][table_name].append({
                        'name': index_name,
                        'sql': create_sql
                    })
                
                logger.info(f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ: {len(schema_info['tables'])}ê°œ í…Œì´ë¸”, {sum(len(indexes) for indexes in schema_info['indexes'].values())}ê°œ ì¸ë±ìŠ¤")
                
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
        
        return schema_info
    
    def create_optimized_indexes(self) -> List[IndexRecommendation]:
        """CSS Picker ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±"""
        recommendations = []
        
        try:
            with self.get_cursor() as cursor:
                # 1. users í…Œì´ë¸” ìµœì í™”
                user_indexes = [
                    # ë¡œê·¸ì¸ ë° ì¸ì¦ ìµœì í™” (ê°€ì¥ ë¹ˆë²ˆí•œ ì¿¼ë¦¬)
                    {
                        'name': 'idx_users_clerk_user_id',
                        'sql': 'CREATE UNIQUE INDEX IF NOT EXISTS idx_users_clerk_user_id ON users(clerk_user_id)',
                        'justification': 'Clerk ì¸ì¦ ì‹œ ì‚¬ìš©ì ì¡°íšŒ ìµœì í™” - ë§¤ìš° ë¹ˆë²ˆí•œ ì¿¼ë¦¬'
                    },
                    # ì´ë©”ì¼ ê²€ìƒ‰ ìµœì í™”
                    {
                        'name': 'idx_users_email',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_users_email ON users(email)',
                        'justification': 'ì´ë©”ì¼ ê¸°ë°˜ ì‚¬ìš©ì ê²€ìƒ‰ ë° ì¤‘ë³µ í™•ì¸ ìµœì í™”'
                    },
                    # í”Œëœë³„ ì‚¬ìš©ì ì¡°íšŒ ìµœì í™”
                    {
                        'name': 'idx_users_plan',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_users_plan ON users(plan)',
                        'justification': 'í”Œëœë³„ ì‚¬ìš©ì í†µê³„ ë° í•„í„°ë§ ìµœì í™”'
                    },
                    # Stripe ê³ ê° ì¡°íšŒ ìµœì í™”
                    {
                        'name': 'idx_users_stripe_customer',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_users_stripe_customer ON users(stripe_customer_id)',
                        'justification': 'ê²°ì œ ì²˜ë¦¬ ì‹œ Stripe ê³ ê° ì¡°íšŒ ìµœì í™”'
                    },
                    # ë³µí•© ì¸ë±ìŠ¤: í”Œëœê³¼ ìƒì„±ì¼ì (ë¶„ì„ ì¿¼ë¦¬ ìµœì í™”)
                    {
                        'name': 'idx_users_plan_created',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_users_plan_created ON users(plan, created_at)',
                        'justification': 'í”Œëœë³„ ê°€ì… ì¶”ì´ ë¶„ì„ ë° ë¦¬í¬íŒ… ìµœì í™”'
                    }
                ]
                
                # 2. payments í…Œì´ë¸” ìµœì í™”
                payment_indexes = [
                    # ì‚¬ìš©ìë³„ ê²°ì œ ë‚´ì—­ ì¡°íšŒ ìµœì í™”
                    {
                        'name': 'idx_payments_user_id',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_payments_user_id ON payments(user_id)',
                        'justification': 'ì‚¬ìš©ìë³„ ê²°ì œ ë‚´ì—­ ì¡°íšŒ ìµœì í™”'
                    },
                    # Stripe ê²°ì œ ID ì¡°íšŒ ìµœì í™”
                    {
                        'name': 'idx_payments_stripe_intent',
                        'sql': 'CREATE UNIQUE INDEX IF NOT EXISTS idx_payments_stripe_intent ON payments(stripe_payment_intent_id)',
                        'justification': 'Stripe ì›¹í›… ì²˜ë¦¬ ì‹œ ì¤‘ë³µ ë°©ì§€ ë° ë¹ ë¥¸ ì¡°íšŒ'
                    },
                    # ê²°ì œ ìƒíƒœë³„ ì¡°íšŒ ìµœì í™”
                    {
                        'name': 'idx_payments_status',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_payments_status ON payments(status)',
                        'justification': 'ì‹¤íŒ¨/ì„±ê³µ ê²°ì œ ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§ ìµœì í™”'
                    },
                    # ë³µí•© ì¸ë±ìŠ¤: ì‚¬ìš©ìì™€ ê²°ì œì¼ì (ê²°ì œ ì´ë ¥ ì¡°íšŒ ìµœì í™”)
                    {
                        'name': 'idx_payments_user_date',
                        'sql': 'CREATE INDEX IF NOT EXISTS idx_payments_user_date ON payments(user_id, payment_date DESC)',
                        'justification': 'ì‚¬ìš©ìë³„ ìµœì‹  ê²°ì œ ë‚´ì—­ ì¡°íšŒ ìµœì í™”'
                    }
                ]
                
                # ì¸ë±ìŠ¤ ìƒì„± ì‹¤í–‰
                all_indexes = user_indexes + payment_indexes
                created_count = 0
                
                for index_info in all_indexes:
                    try:
                        start_time = time.time()
                        cursor.execute(index_info['sql'])
                        execution_time = (time.time() - start_time) * 1000
                        
                        # ì¶”ì²œ ê°ì²´ ìƒì„±
                        recommendation = IndexRecommendation(
                            table_name=index_info['name'].split('_')[1],  # Extract table name
                            columns=[col.strip() for col in index_info['sql'].split('(')[1].split(')')[0].split(',')],
                            index_type='UNIQUE' if 'UNIQUE' in index_info['sql'] else 'COMPOSITE' if ',' in index_info['sql'] else 'SINGLE',
                            estimated_improvement=0.85,  # ì˜ˆìƒ 85% ì„±ëŠ¥ í–¥ìƒ
                            creation_cost=f"{execution_time:.2f}ms",
                            maintenance_overhead='ë‚®ìŒ',
                            justification=index_info['justification']
                        )
                        
                        recommendations.append(recommendation)
                        created_count += 1
                        
                        logger.info(f"ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ: {index_info['name']} ({execution_time:.2f}ms)")
                        
                    except Exception as e:
                        if 'already exists' not in str(e).lower():
                            logger.error(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ {index_info['name']}: {e}")
                
                self.db.commit()
                logger.info(f"ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ: {created_count}/{len(all_indexes)}ê°œ ì¸ë±ìŠ¤ ìƒì„±")
                
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            raise
        
        return recommendations
    
    def analyze_query_performance(self) -> List[QueryAnalysis]:
        """CSS Picker í•µì‹¬ ì¿¼ë¦¬ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„"""
        core_queries = [
            # ì‚¬ìš©ì ì¸ì¦ ì¿¼ë¦¬ (ê°€ì¥ ë¹ˆë²ˆ)
            {
                'name': 'user_auth_lookup',
                'query': 'SELECT id, clerk_user_id, email, plan, premium_activated_at, stripe_customer_id, created_at, updated_at FROM users WHERE clerk_user_id = ?',
                'params': ['user_test_123']
            },
            # ì´ë©”ì¼ ê¸°ë°˜ ì‚¬ìš©ì ì¡°íšŒ
            {
                'name': 'user_email_lookup',
                'query': 'SELECT id, plan FROM users WHERE email = ?',
                'params': ['test@example.com']
            },
            # ê²°ì œ ë‚´ì—­ ì¡°íšŒ
            {
                'name': 'payment_history',
                'query': 'SELECT * FROM payments WHERE user_id = ? ORDER BY payment_date DESC LIMIT 10',
                'params': ['usr_test123']
            },
            # Stripe ê³ ê° ID ì¡°íšŒ
            {
                'name': 'stripe_customer_lookup',
                'query': 'SELECT id, stripe_customer_id FROM users WHERE id = ?',
                'params': ['usr_test123']
            },
            # í”Œëœë³„ ì‚¬ìš©ì í†µê³„
            {
                'name': 'plan_statistics',
                'query': 'SELECT plan, COUNT(*) as user_count FROM users GROUP BY plan',
                'params': []
            },
            # ìµœê·¼ ê°€ì… ì‚¬ìš©ì
            {
                'name': 'recent_users',
                'query': 'SELECT id, email, plan, created_at FROM users WHERE created_at >= ? ORDER BY created_at DESC LIMIT 50',
                'params': [(datetime.now() - timedelta(days=7)).isoformat()]
            }
        ]
        
        analyses = []
        
        try:
            with self.get_cursor() as cursor:
                for query_info in core_queries:
                    try:
                        # EXPLAIN QUERY PLANìœ¼ë¡œ ì¿¼ë¦¬ ë¶„ì„
                        explain_query = f"EXPLAIN QUERY PLAN {query_info['query']}"
                        cursor.execute(explain_query, query_info['params'])
                        explanation = ' | '.join([str(row) for row in cursor.fetchall()])
                        
                        # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                        start_time = time.time()
                        cursor.execute(query_info['query'], query_info['params'])
                        results = cursor.fetchall()
                        execution_time = (time.time() - start_time) * 1000
                        
                        # ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ
                        suggestions = []
                        severity = 'LOW'
                        index_candidates = []
                        
                        if execution_time > 100:  # 100ms ì´ìƒì´ë©´ ëŠë¦° ì¿¼ë¦¬
                            severity = 'HIGH' if execution_time > 500 else 'MEDIUM'
                            suggestions.append('ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ì–´ ì¸ë±ìŠ¤ ì¶”ê°€ ê²€í†  í•„ìš”')
                        
                        if 'SCAN' in explanation.upper():
                            suggestions.append('í…Œì´ë¸” í’€ ìŠ¤ìº” ë°œìƒ - ì ì ˆí•œ ì¸ë±ìŠ¤ ë¶€ì¬')
                            severity = 'HIGH'
                        
                        if query_info['name'] == 'user_auth_lookup':
                            index_candidates.append('idx_users_clerk_user_id')
                        elif query_info['name'] == 'user_email_lookup':
                            index_candidates.append('idx_users_email')
                        elif query_info['name'] == 'payment_history':
                            index_candidates.append('idx_payments_user_date')
                        
                        analysis = QueryAnalysis(
                            query=query_info['query'],
                            execution_time_ms=execution_time,
                            rows_affected=len(results),
                            explanation=explanation,
                            optimization_suggestions=suggestions,
                            index_candidates=index_candidates,
                            severity=severity
                        )
                        
                        analyses.append(analysis)
                        
                        logger.info(f"ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: {query_info['name']} - {execution_time:.2f}ms ({severity})")
                        
                    except Exception as e:
                        logger.error(f"ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨ {query_info['name']}: {e}")
                
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
        
        return analyses
    
    def setup_backup_automation(self) -> Dict[str, Any]:
        """ìë™ ë°±ì—… ì‹œìŠ¤í…œ êµ¬ì¶•"""
        backup_config = {
            'strategy': 'incremental_with_full_weekly',
            'retention_policy': '7_daily_4_weekly_12_monthly',
            'backup_location': 'turso_native_backup',
            'monitoring': 'enabled'
        }
        
        try:
            # Turso CLIë¥¼ ì‚¬ìš©í•œ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
            backup_script = self._generate_backup_script()
            
            with open('backup_automation.sh', 'w') as f:
                f.write(backup_script)
            
            logger.info("ë°±ì—… ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ")
            
            # ë°±ì—… ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í…Œì´ë¸” ìƒì„±
            with self.get_cursor() as cursor:
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS backup_logs (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        backup_type TEXT NOT NULL,
                        backup_size_bytes INTEGER,
                        backup_location TEXT,
                        status TEXT NOT NULL,
                        started_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                        completed_at DATETIME,
                        error_message TEXT
                    )
                """)
                
                # ë°±ì—… ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì¸ë±ìŠ¤
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_backup_logs_status_date 
                    ON backup_logs(status, started_at DESC)
                """)
                
                self.db.commit()
            
            backup_config['script_created'] = True
            backup_config['monitoring_table'] = 'backup_logs'
            
        except Exception as e:
            logger.error(f"ë°±ì—… ìë™í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            backup_config['error'] = str(e)
        
        return backup_config
    
    def _generate_backup_script(self) -> str:
        """ë°±ì—… ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        return '''#!/bin/bash
# CSS Picker Turso ë°ì´í„°ë² ì´ìŠ¤ ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
# ìƒì„±ì¼: ''' + datetime.now().isoformat() + '''

DB_NAME="css-picker"
BACKUP_DIR="/var/backups/css-picker"
LOG_FILE="/var/log/css-picker-backup.log"
RETENTION_DAYS=30

# ë¡œê·¸ í•¨ìˆ˜
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_DIR"

# ì¼ì¼ ë°±ì—… (Monday-Friday)
if [[ $(date +%u) -le 5 ]]; then
    BACKUP_FILE="${BACKUP_DIR}/daily-$(date +%Y%m%d).db"
    log "ì¼ì¼ ë°±ì—… ì‹œì‘: $BACKUP_FILE"
    
    turso db dump "$DB_NAME" --output "$BACKUP_FILE"
    
    if [ $? -eq 0 ]; then
        log "ì¼ì¼ ë°±ì—… ì„±ê³µ: $(du -h $BACKUP_FILE | cut -f1)"
    else
        log "ERROR: ì¼ì¼ ë°±ì—… ì‹¤íŒ¨"
        exit 1
    fi
fi

# ì£¼ê°„ ì „ì²´ ë°±ì—… (Sunday)
if [[ $(date +%u) -eq 7 ]]; then
    BACKUP_FILE="${BACKUP_DIR}/weekly-$(date +%Y%U).db"
    log "ì£¼ê°„ ë°±ì—… ì‹œì‘: $BACKUP_FILE"
    
    turso db dump "$DB_NAME" --output "$BACKUP_FILE"
    
    if [ $? -eq 0 ]; then
        log "ì£¼ê°„ ë°±ì—… ì„±ê³µ: $(du -h $BACKUP_FILE | cut -f1)"
    else
        log "ERROR: ì£¼ê°„ ë°±ì—… ì‹¤íŒ¨"
        exit 1
    fi
fi

# ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
find "$BACKUP_DIR" -name "daily-*.db" -mtime +7 -delete
find "$BACKUP_DIR" -name "weekly-*.db" -mtime +"$RETENTION_DAYS" -delete

log "ë°±ì—… ì •ë¦¬ ì™„ë£Œ"

# ë°±ì—… ë¬´ê²°ì„± ê²€ì‚¬
LATEST_BACKUP=$(ls -t "$BACKUP_DIR"/*.db | head -1)
if [ -f "$LATEST_BACKUP" ]; then
    sqlite3 "$LATEST_BACKUP" "PRAGMA integrity_check;" > /dev/null
    if [ $? -eq 0 ]; then
        log "ë°±ì—… ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼: $LATEST_BACKUP"
    else
        log "WARNING: ë°±ì—… ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨: $LATEST_BACKUP"
    fi
fi

log "ë°±ì—… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ"
'''
    
    def generate_optimization_report(self) -> OptimizationReport:
        """ì¢…í•© ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¸¡ì •
            with self.get_cursor() as cursor:
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                db_size_bytes = cursor.fetchone()[0]
                db_size_mb = db_size_bytes / (1024 * 1024)
                
                # í…Œì´ë¸” ìˆ˜ ê³„ì‚°
                cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type = 'table' AND name NOT LIKE 'sqlite_%'")
                table_count = cursor.fetchone()[0]
                
                # ì¸ë±ìŠ¤ ìˆ˜ ê³„ì‚°
                cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND name NOT LIKE 'sqlite_%'")
                index_count = cursor.fetchone()[0]
            
            # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
            slow_queries = self.analyze_query_performance()
            
            # ì¸ë±ìŠ¤ ìµœì í™” ì‹¤í–‰
            index_recommendations = self.create_optimized_indexes()
            
            # ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •
            backup_info = self.setup_backup_automation()
            
            # ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
            performance_metrics = {
                'avg_query_time': sum(q.execution_time_ms for q in slow_queries) / max(len(slow_queries), 1),
                'slow_query_count': len([q for q in slow_queries if q.severity in ['HIGH', 'CRITICAL']]),
                'index_coverage': len(index_recommendations),
                'optimization_score': self._calculate_optimization_score(slow_queries, index_recommendations)
            }
            
            report = OptimizationReport(
                timestamp=datetime.now().isoformat(),
                database_size_mb=db_size_mb,
                total_tables=table_count,
                total_indexes=index_count,
                slow_queries=slow_queries,
                index_recommendations=index_recommendations,
                performance_metrics=performance_metrics,
                backup_info=backup_info
            )
            
            logger.info("ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            return report
            
        except Exception as e:
            logger.error(f"ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _calculate_optimization_score(self, queries: List[QueryAnalysis], indexes: List[IndexRecommendation]) -> float:
        """ìµœì í™” ì ìˆ˜ ê³„ì‚° (0-100ì )"""
        base_score = 70.0
        
        # ëŠë¦° ì¿¼ë¦¬ì— ëŒ€í•œ ê°ì 
        high_severity_queries = len([q for q in queries if q.severity == 'HIGH'])
        critical_queries = len([q for q in queries if q.severity == 'CRITICAL'])
        
        query_penalty = (high_severity_queries * 5) + (critical_queries * 10)
        
        # ì¸ë±ìŠ¤ ìµœì í™”ì— ëŒ€í•œ ê°€ì 
        index_bonus = len(indexes) * 3
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_score = base_score - query_penalty + index_bonus
        return max(0, min(100, final_score))
    
    def save_report(self, report: OptimizationReport, filename: str = None):
        """ìµœì í™” ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"database_optimization_report_{timestamp}.json"
        
        try:
            # ë³´ê³ ì„œë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
            report_dict = {
                'timestamp': report.timestamp,
                'database_info': {
                    'size_mb': report.database_size_mb,
                    'total_tables': report.total_tables,
                    'total_indexes': report.total_indexes
                },
                'performance_metrics': report.performance_metrics,
                'slow_queries': [
                    {
                        'query': q.query,
                        'execution_time_ms': q.execution_time_ms,
                        'rows_affected': q.rows_affected,
                        'severity': q.severity,
                        'suggestions': q.optimization_suggestions
                    } for q in report.slow_queries
                ],
                'index_recommendations': [
                    {
                        'table_name': idx.table_name,
                        'columns': idx.columns,
                        'index_type': idx.index_type,
                        'estimated_improvement': idx.estimated_improvement,
                        'justification': idx.justification
                    } for idx in report.index_recommendations
                ],
                'backup_info': report.backup_info
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ìµœì í™” ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

def run_complete_optimization():
    """ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í–‰"""
    from dotenv import load_dotenv
    load_dotenv()
    
    database_url = os.getenv('TURSO_DATABASE_URL')
    auth_token = os.getenv('TURSO_AUTH_TOKEN')
    
    if not database_url or not auth_token:
        logger.error("TURSO_DATABASE_URL ë˜ëŠ” TURSO_AUTH_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False
    
    try:
        logger.info("ğŸš€ CSS Picker ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹œì‘")
        
        optimizer = TursoOptimizer(database_url, auth_token)
        
        # í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë¶„ì„
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘...")
        schema_info = optimizer.analyze_current_schema()
        
        # ì¢…í•© ìµœì í™” ì‹¤í–‰
        logger.info("âš¡ ì¢…í•© ìµœì í™” ì‹¤í–‰ ì¤‘...")
        report = optimizer.generate_optimization_report()
        
        # ë³´ê³ ì„œ ì €ì¥
        optimizer.save_report(report)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ¯ CSS Picker ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“ˆ ìµœì í™” ì ìˆ˜: {report.performance_metrics['optimization_score']:.1f}/100")
        print(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {report.database_size_mb:.2f} MB")
        print(f"ğŸ—ï¸ í…Œì´ë¸” ìˆ˜: {report.total_tables}ê°œ")
        print(f"ğŸ“‡ ì¸ë±ìŠ¤ ìˆ˜: {report.total_indexes}ê°œ")
        print(f"ğŸ” ìƒì„±ëœ ì¸ë±ìŠ¤: {len(report.index_recommendations)}ê°œ")
        print(f"âš ï¸ ëŠë¦° ì¿¼ë¦¬: {report.performance_metrics['slow_query_count']}ê°œ")
        print(f"â±ï¸ í‰ê·  ì¿¼ë¦¬ ì‹œê°„: {report.performance_metrics['avg_query_time']:.2f}ms")
        
        if report.backup_info.get('script_created'):
            print("ğŸ’¾ ë°±ì—… ìë™í™”: âœ… êµ¬ì„± ì™„ë£Œ")
        else:
            print("ğŸ’¾ ë°±ì—… ìë™í™”: âŒ ì˜¤ë¥˜ ë°œìƒ")
        
        print("\nğŸ“Š ì£¼ìš” ìµœì í™” í•­ëª©:")
        for idx in report.index_recommendations[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
            print(f"  â€¢ {idx.table_name} í…Œì´ë¸”: {idx.justification}")
        
        if report.performance_metrics['optimization_score'] >= 85:
            print("\nğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìµœì í™” ìƒíƒœì…ë‹ˆë‹¤!")
        elif report.performance_metrics['optimization_score'] >= 70:
            print("\nâœ… ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ì¶”ê°€ ìµœì í™”ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = run_complete_optimization()
    exit(0 if success else 1)